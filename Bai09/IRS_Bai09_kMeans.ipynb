{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Cài đặt thư viện UnderTheSea để hỗ trợ tách từ trong tiếng Việt. Xem thêm các tính năng của thư viện tại: https://github.com/undertheseanlp/underthesea**"
      ],
      "metadata": {
        "id": "vI3LKjVI1Not"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX4i0cBg1Pg8",
        "outputId": "c6269718-b372-44b8-951c-ced1fed7570f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting underthesea\n",
            "  Downloading underthesea-1.4.0-py3-none-any.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from underthesea) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from underthesea) (1.2.0)\n",
            "Collecting underthesea-core==0.0.5a2\n",
            "  Downloading underthesea_core-0.0.5_alpha.2-cp38-cp38-manylinux2010_x86_64.whl (591 kB)\n",
            "\u001b[K     |████████████████████████████████| 591 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.8/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from underthesea) (4.64.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from underthesea) (1.0.2)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from underthesea) (6.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->underthesea) (2022.6.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.8 underthesea-1.4.0 underthesea-core-0.0.5a2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import các thư viện cần thiết**"
      ],
      "metadata": {
        "id": "x2VdLXqKfp60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from underthesea import text_normalize\n",
        "from underthesea import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "xNoI_30EfqTr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lấy dữ liệu các tài liệu/văn bản mẫu từ**: https://github.com/HUTECH-OpenCourseWare/IRS.git"
      ],
      "metadata": {
        "id": "D9Q93wNN1mFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HUTECH-OpenCourseWare/IRS.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS8TSt6m1m7r",
        "outputId": "e71fb03c-c56a-4f6c-9561-8571353a2edf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IRS'...\n",
            "remote: Enumerating objects: 271, done.\u001b[K\n",
            "remote: Counting objects: 100% (271/271), done.\u001b[K\n",
            "remote: Compressing objects: 100% (271/271), done.\u001b[K\n",
            "remote: Total 271 (delta 0), reused 271 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (271/271), 441.48 KiB | 16.35 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành thử nghiệm với danh sách các tài liệu/văn bản thuộc các chủ đề khác nhau**"
      ],
      "metadata": {
        "id": "KA5-KE4f1YmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chọn danh sách các chủ đề của tài liệu/văn bản cho thử nghiệm\n",
        "topics = [\n",
        "    'the-thao',\n",
        "    'du-lich',\n",
        "    'khoa-hoc'\n",
        "]\n",
        "\n",
        "number_of_topics = len(topics)\n",
        "\n",
        "# Với mỗi chủ đề - chúng ta sẽ giới hạn số lượng tài liệu/văn bản \n",
        "# Nhằm đảm bảo đồng đều về số lượng tài liệu/văn bản cho từng chủ đề\n",
        "limit_doc_per_topic = 5\n",
        "\n",
        "# Tạo một tập dữ liệu thử nghiệm gồm các tài liệu/văn bản thuộc về 2-3 chủ đề\n",
        "# Cấu trúc dữ liệu dạng list - lưu thông tin danh sách các tài liệu/văn bản thuộc chủ đề khác nhau\n",
        "# Mỗi tài liệu/văn bản sẽ tổ chức dạng 1 tuple với: (topic, nội_dung_văn_bản, danh_sách_token)\n",
        "D = []"
      ],
      "metadata": {
        "id": "3uAtKKYk1W0s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành viết một số hàm hỗ trợ cho việc đọc dữ liệu, xử lý và tách từ trong tiếng Việt.**"
      ],
      "metadata": {
        "id": "DAW8o4vE1e97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Đọc danh sách các từ stopwords trong tiếng Việt\n",
        "vn_stopwords_file_path = '/content/IRS/data/stopwords/vietnamese-stopwords.txt'\n",
        "stopwords = []\n",
        "with open(vn_stopwords_file_path, 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    line = line.strip()\n",
        "    stopwords.append(line)\n",
        "\n",
        "# Viết hàm tiền xử lý và tách từ tiếng Việt\n",
        "def preprocess(doc):\n",
        "  # Tiến hành xử lý các lỗi từ/câu, dấu câu, v.v. trong tiếng Việt với hàm text_normalize\n",
        "  normalized_doc = text_normalize(doc)\n",
        "  # Tiến hành tách từ\n",
        "  tokens = word_tokenize(normalized_doc)\n",
        "  # Lọc stopwords\n",
        "  non_stopword_tokens = []\n",
        "  for token in tokens:\n",
        "    if token not in stopwords:\n",
        "      non_stopword_tokens.append(token)\n",
        "  # Tiến hành kết hợp các từ ghép trong tiếng Việt bằng '_'\n",
        "  combined_tokens = [token.replace(' ', '_') for token in non_stopword_tokens]\n",
        "  return (normalized_doc, combined_tokens)\n",
        "\n",
        "# Viết hàm lấy danh sách các văn bản/tài liệu thuộc các chủ đề khác nhau\n",
        "def fetch_doc_by_topic(topic, limit = 10):\n",
        "  data_root_dir_path = '/content/IRS/data/vnexpress/{}'.format(topic)\n",
        "  docs = []\n",
        "  doc_count = 0\n",
        "  for file_name in os.listdir(data_root_dir_path):\n",
        "    if doc_count >= limit:\n",
        "      break\n",
        "    file_path = os.path.join(data_root_dir_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "      lines = []\n",
        "      for line in f:\n",
        "        line = line.lower().strip()\n",
        "        lines.append(line)\n",
        "    doc = \" \".join(lines)\n",
        "    clean_doc = re.sub('\\W+',' ', doc)\n",
        "    (normalized_doc, tokens) = preprocess(clean_doc)\n",
        "    docs.append((topic, normalized_doc, tokens))\n",
        "    doc_count+=1\n",
        "  return docs"
      ],
      "metadata": {
        "id": "oKmmm_hQ1bgL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành tạo tập dữ liệu thử nghiệm với các tài liệu/văn bản thuộc danh sách chủ đề [topics] đã lựa chọn bên trên**"
      ],
      "metadata": {
        "id": "TdCueZIW1rVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu trúc dữ liệu dictionary để lưu thông tin chủ đề-tài liệu, nhằm hỗ trợ cho việc tìm kiếm nhanh\n",
        "topic_doc_idxes_dict = {}\n",
        "doc_idx_topic_dict = {}\n",
        "\n",
        "# Duyệt qua từng chủ đề\n",
        "doc_idx = 0\n",
        "for topic in topics:\n",
        "  current_topic_docs = fetch_doc_by_topic(topic, limit_doc_per_topic)\n",
        "  topic_doc_idxes_dict[topic] = []\n",
        "  for (topic, normalized_doc, tokens) in current_topic_docs:\n",
        "    topic_doc_idxes_dict[topic].append(doc_idx)\n",
        "    doc_idx_topic_dict[doc_idx] = topic\n",
        "    doc_idx+=1\n",
        "  D += current_topic_docs\n",
        "\n",
        "doc_size = len(D)\n",
        "\n",
        "print('Hoàn tất, tổng số lượng tài liệu/văn bản đã lấy: [{}]'.format(doc_size))\n",
        "for topic in topic_doc_idxes_dict.keys():\n",
        "  print(' - Chủ đề [{}] có [{}] tài liệu/văn bản.'.format(topic, len(topic_doc_idxes_dict[topic])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hOoa9xu1vaD",
        "outputId": "34db65ea-ecfa-4a11-e553-3fcabc0a2e27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hoàn tất, tổng số lượng tài liệu/văn bản đã lấy: [15]\n",
            " - Chủ đề [the-thao] có [5] tài liệu/văn bản.\n",
            " - Chủ đề [du-lich] có [5] tài liệu/văn bản.\n",
            " - Chủ đề [khoa-hoc] có [5] tài liệu/văn bản.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành biến đổi các tài liệu/văn bản trong tập (D) về dạng các TF-IDF vectors - trong bài thực hành này chúng ta sẽ sử dụng thư viện Scikit-Learn (TfidfVectorizer) https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html**"
      ],
      "metadata": {
        "id": "FwpTnsEK1yYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo đối tượng TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Chúng ta sẽ tạo ra một tập danh sách các tài liệu/văn bản dạng list đơn giản để thư viện Scikit-Learn có thể đọc được\n",
        "sk_docs = []\n",
        "\n",
        "# Duyệt qua từng tài liệu/văn bản có trong (D)\n",
        "for (topic, normalized_doc, tokens) in D:\n",
        "  # Chúng ta sẽ nối các từ/tokens đã được tách để làm thành một văn bản hoàn chỉnh\n",
        "  text = ' '.join(tokens)\n",
        "  sk_docs.append(text)\n",
        "\n",
        "# Tiến hành chuyển đổi các tài liệu/văn bản về dạng các TF-IDF vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(sk_docs)\n",
        "print(f'Kích thước ma trận tf-idf: [{tfidf_matrix.shape}]')\n",
        "\n",
        "# Chuyển ma trận tfidf_matrix từ dạng cấu trúc thưa sang dạng đầy đủ để thuận tiện cho việc tính toán\n",
        "tfidf_matrix = tfidf_matrix.todense().tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icK715IW1zXL",
        "outputId": "3b938cc2-f684-4b73-d964-284cee168bfb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kích thước ma trận tf-idf: [(15, 1843)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành chuẩn bị tập dữ liệu cho bài toán gom cụm với thuật toán K-means, với đầu vào sẽ là danh sách các tài liệu/văn bản đã được chuyển đổi về dạng các vector TF-IDF**"
      ],
      "metadata": {
        "id": "w6JNKRXe2B3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo một danh sách chứa thông tin về mã định danh tài liệu (doc_idx) và tf-idf vector tương ứng của nó\n",
        "docs = []\n",
        "for doc_idx, doc_tfidf_vector in enumerate(tfidf_matrix):\n",
        "  docs.append((doc_idx, doc_tfidf_vector))"
      ],
      "metadata": {
        "id": "MiND3EZZ2RgR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành cài đặt thuật toán k-means, trong đó điểm trọng tâm (centroid) của một cụm ($c$), ký hiệu $\\vec{μ}(c)$ sẽ được xác định như sau:**\n",
        "# $$\\vec{μ}(c)=\\frac{1}{|c|}\\sum_{\\vec{x}∈c}\\vec(x)$$"
      ],
      "metadata": {
        "id": "DRx5-ySmf28j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "enz80BwufVR9"
      },
      "outputs": [],
      "source": [
        "#Function to implement steps given in previous section\n",
        "class KMeans_Algorithm():\n",
        "  def __init__(self, docs, k, max_iterations):\n",
        "        self.docs = np.asarray(docs)\n",
        "        self.doc_idxes = np.asarray([doc_idx for (doc_idx, doc_vector) in self.docs])\n",
        "        self.doc_vectors = np.asarray([doc_vector for (doc_idx, doc_vector) in self.docs])\n",
        "        self.N = len(docs)\n",
        "        self.k = k\n",
        "        self.max_iterations = max_iterations\n",
        "        self.clusters = None\n",
        "  \n",
        "  # Hàm chính chạy thuật toán\n",
        "  def run(self):\n",
        "\n",
        "    # Lựa chọn ngẫu nhiên vị trí của các điểm trung tâm (centroids) của các cụm bằng việc chọn ngẫu nhiên một số lượng tài liệu/văn bản bằng số (l)\n",
        "    random_selected_doc_idxes = np.random.choice(len(self.doc_vectors), self.k, replace=False)\n",
        "    # Dùng (k) các văn bản/tài liệu lựa chọn ngẫu nhiên trên để làm centroids ban đầu\n",
        "    centroids = self.doc_vectors[random_selected_doc_idxes, :]\n",
        "     \n",
        "    # Chúng ta sẽ dùng hàm cdist của thư viện scipy để tính khoảng cách của các tài liệu/văn bản đến các (centroids)\n",
        "    # Kết quả trả về sẽ là một ma trận khoảng cách có kích thước: n x k - trong đó n: số lượng tài liệu/văn bản và (k) là số lượng cụm.\n",
        "    #Ví dụ: với (4) tài liệu và (3) cụm, ta sẽ có ma trận khoảng cách như sau\n",
        "    #  [[17.49285568  0.          8.60232527]\n",
        "    #   [ 9.2736185   8.60232527  0.        ]\n",
        "    #   [ 3.         15.58845727  8.06225775]\n",
        "    #   [ 0.         17.49285568  9.2736185 ]]\n",
        "    # Ở đây chúng ta sẽ sử dụng khoản cách Euclid\n",
        "    dists = distance.cdist(self.doc_vectors, centroids ,'euclidean')\n",
        "     \n",
        "    # Chúng ta tiến hành xác định cụm của các tài liệu/văn bản dựa trên khoảng cách nhỏ nhất\n",
        "    # Cấu trúc ở dạng list - với kích thước = số lượng tài liệu/văn bản, ví dụ với k = 3: [2 1 0 0] \n",
        "    self.clusters = np.array([np.argmin(dist) for dist in dists])\n",
        "  \n",
        "\n",
        "    # Tiến hành cập nhật lại điểm trọng tâm/centroids của các cụm với 1 số lần lặp [max_iterations]\n",
        "    # Quá trình cập nhật sẽ bao gồm: \n",
        "    # BƯỚC 1: tính điểm trọng tâm/centroid mới - dựa trên tính trung bình khoảng cách giữa các tài liệu/văn bản\n",
        "    # BƯỚC 2: tính lại khoảng cách giữa các tài liệu/văn bản cho các centroids mới \n",
        "    # BƯỚC 3: Cập nhật lại thông tin cụm cho các tài liệu/văn bản dựa trên khoảng cách với các centroids mới\n",
        "    for _ in range(self.max_iterations): \n",
        "\n",
        "        # BƯỚC 1: tính điểm trọng tâm/centroid mới\n",
        "        new_centroids = []\n",
        "        for cluster_idx in range(self.k):\n",
        "            # Chúng ta tính khoảng cách trung bình bằng hàm mean(axis=0) - theo hàng và axis=1 là theo cột\n",
        "            # Ví dụ, ta có: matrix = np.asarray([\n",
        "            #  [1, 2, 3],\n",
        "            #  [4, 5, 6],\n",
        "            #  [6, 8, 9],\n",
        "            # [10, 11, 12]\n",
        "            #])\n",
        "            # matrix.mean(axis=0) -> array([5.25, 6.5 , 7.5 ])\n",
        "            # matrix.mean(axis=1) -> array([ 2.,  5., 7.66666667, 11.])\n",
        "            new_centroid_for_cluster_idx = self.doc_vectors[self.clusters==cluster_idx].mean(axis=0) \n",
        "            new_centroids.append(new_centroid_for_cluster_idx)\n",
        "        \n",
        "        # Cập nhật lại các điểm trọng tâm/centrods\n",
        "        # Chuẩn hóa lại dữ liệu các centroids từ dạng danh sách: [array([1., 2., 3.]), array([10. , 11. , 13.5]), array([ 5.,  5., 10.])]\n",
        "        # Thành dạng ma trận numpy nhằm thuận tiện cho việc tính toán: \n",
        "        # [[ 1.   2.   3. ]\n",
        "        #  [10.  11.  13.5]\n",
        "        #  [ 5.   5.  10. ]]\n",
        "        centroids = np.vstack(new_centroids)\n",
        "\n",
        "        # BƯỚC 2: tính lại khoảng cách giữa các tài liệu/văn bản cho các centroids mới\n",
        "        dists = distance.cdist(self.doc_vectors, centroids ,'euclidean') #Step 2\n",
        "\n",
        "        # BƯỚC 3: Cập nhật lại thông tin cụm cho các tài liệu/văn bản dựa trên khoảng cách với các centroids mới\n",
        "        self.clusters = np.array([np.argmin(dist) for dist in dists])\n",
        "\n",
        "  # Xuất thông tin các cụm sau khi hoàn tất\n",
        "  def show_cluster_info(self):\n",
        "    for cluster_idx in range(self.k):\n",
        "      print(\"Cụm (cluster) [{}]:\".format(cluster_idx))     \n",
        "      for doc_idx in self.doc_idxes[self.clusters==cluster_idx]:\n",
        "        print(f\" - Chủ đề: [{doc_idx_topic_dict[doc_idx]}]/mã tài liệu: [{doc_idx}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành chạy thuật toán K-means với tập dữ liệu thuộc các chủ đề khác trong nhau tập [docs] - chúng ta sẽ lấy giá trị số lượng cụm kỳ vọng bằng với số lượng chủ đề - ví dụ: k = 3**"
      ],
      "metadata": {
        "id": "ugEcgEw05Gmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khai báo số lượng cụm kỳ vọng (k)\n",
        "k = number_of_topics\n",
        "\n",
        "max_iterations = 100\n",
        "\n",
        "# Khởi tạo & khai báo các tham số cho mô hình gom cụm HAC \n",
        "kmeans_algorithm = KMeans_Algorithm(docs, k, max_iterations)\n",
        "\n",
        "# Tiến hành chạy thuật toán HAC\n",
        "print(f'Tiến hành chạy thuật toán KMeans với tập dữ liệu: [{len(docs)}] tài liệu/văn bản - số cụm kỳ vọng (k) = [{k}]...')\n",
        "kmeans_algorithm.run()\n",
        "\n",
        "# Hoàn tất - xuất kết quả thông tin các cụm\n",
        "print('Hoàn tất ! xuất thông tin các cụm...')\n",
        "kmeans_algorithm.show_cluster_info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7UFFJVehDhh",
        "outputId": "3e2e0018-c71d-4620-e702-f870d7178d0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiến hành chạy thuật toán KMeans với tập dữ liệu: [15] tài liệu/văn bản - số cụm kỳ vọng (k) = [3]...\n",
            "Hoàn tất ! xuất thông tin các cụm...\n",
            "Cụm (cluster) [0]:\n",
            " - Chủ đề: [khoa-hoc]/mã tài liệu: [11]\n",
            " - Chủ đề: [khoa-hoc]/mã tài liệu: [14]\n",
            "Cụm (cluster) [1]:\n",
            " - Chủ đề: [du-lich]/mã tài liệu: [5]\n",
            " - Chủ đề: [du-lich]/mã tài liệu: [6]\n",
            " - Chủ đề: [du-lich]/mã tài liệu: [8]\n",
            " - Chủ đề: [khoa-hoc]/mã tài liệu: [10]\n",
            " - Chủ đề: [khoa-hoc]/mã tài liệu: [12]\n",
            " - Chủ đề: [khoa-hoc]/mã tài liệu: [13]\n",
            "Cụm (cluster) [2]:\n",
            " - Chủ đề: [the-thao]/mã tài liệu: [0]\n",
            " - Chủ đề: [the-thao]/mã tài liệu: [1]\n",
            " - Chủ đề: [the-thao]/mã tài liệu: [2]\n",
            " - Chủ đề: [the-thao]/mã tài liệu: [3]\n",
            " - Chủ đề: [the-thao]/mã tài liệu: [4]\n",
            " - Chủ đề: [du-lich]/mã tài liệu: [7]\n",
            " - Chủ đề: [du-lich]/mã tài liệu: [9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-21d572b25428>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.docs = np.asarray(docs)\n"
          ]
        }
      ]
    }
  ]
}