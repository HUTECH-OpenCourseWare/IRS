{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Cài đặt thư viện UnderTheSea để hỗ trợ tách từ trong tiếng Việt. Xem thêm các tính năng của thư viện tại: https://github.com/undertheseanlp/underthesea**"
      ],
      "metadata": {
        "id": "vI3LKjVI1Not"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX4i0cBg1Pg8",
        "outputId": "3430617d-0855-4eac-9660-37d4da463ac3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting underthesea\n",
            "  Downloading underthesea-1.4.0-py3-none-any.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 4.7 MB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 46.2 MB/s \n",
            "\u001b[?25hCollecting underthesea-core==0.0.5a2\n",
            "  Downloading underthesea_core-0.0.5_alpha.2-cp38-cp38-manylinux2010_x86_64.whl (591 kB)\n",
            "\u001b[K     |████████████████████████████████| 591 kB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from underthesea) (6.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.8/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from underthesea) (4.64.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from underthesea) (3.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from underthesea) (1.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from underthesea) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->underthesea) (2022.6.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (2022.9.24)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.8 underthesea-1.4.0 underthesea-core-0.0.5a2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import các thư viện cần thiết**"
      ],
      "metadata": {
        "id": "x2VdLXqKfp60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from underthesea import text_normalize\n",
        "from underthesea import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "xNoI_30EfqTr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lấy dữ liệu các tài liệu/văn bản mẫu từ**: https://github.com/HUTECH-OpenCourseWare/IRS.git"
      ],
      "metadata": {
        "id": "D9Q93wNN1mFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HUTECH-OpenCourseWare/IRS.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS8TSt6m1m7r",
        "outputId": "7b1c5aa4-ca4f-4816-8240-f16f40f86dc1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'IRS' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành thử nghiệm với danh sách các tài liệu/văn bản thuộc các chủ đề khác nhau**"
      ],
      "metadata": {
        "id": "KA5-KE4f1YmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chọn danh sách các chủ đề của tài liệu/văn bản cho thử nghiệm\n",
        "topics = [\n",
        "    'the-thao',\n",
        "    'du-lich',\n",
        "    'khoa-hoc'\n",
        "]\n",
        "\n",
        "number_of_topics = len(topics)\n",
        "\n",
        "# Với mỗi chủ đề - chúng ta sẽ giới hạn số lượng tài liệu/văn bản \n",
        "# Nhằm đảm bảo đồng đều về số lượng tài liệu/văn bản cho từng chủ đề\n",
        "limit_doc_per_topic = 5\n",
        "\n",
        "# Tạo một tập dữ liệu thử nghiệm gồm các tài liệu/văn bản thuộc về 2-3 chủ đề\n",
        "# Cấu trúc dữ liệu dạng list - lưu thông tin danh sách các tài liệu/văn bản thuộc chủ đề khác nhau\n",
        "# Mỗi tài liệu/văn bản sẽ tổ chức dạng 1 tuple với: (topic, nội_dung_văn_bản, danh_sách_token)\n",
        "D = []"
      ],
      "metadata": {
        "id": "3uAtKKYk1W0s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành viết một số hàm hỗ trợ cho việc đọc dữ liệu, xử lý và tách từ trong tiếng Việt.**"
      ],
      "metadata": {
        "id": "DAW8o4vE1e97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Đọc danh sách các từ stopwords trong tiếng Việt\n",
        "vn_stopwords_file_path = '/content/IRS/data/stopwords/vietnamese-stopwords.txt'\n",
        "stopwords = []\n",
        "with open(vn_stopwords_file_path, 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    line = line.strip()\n",
        "    stopwords.append(line)\n",
        "\n",
        "# Viết hàm tiền xử lý và tách từ tiếng Việt\n",
        "def preprocess(doc):\n",
        "  # Tiến hành xử lý các lỗi từ/câu, dấu câu, v.v. trong tiếng Việt với hàm text_normalize\n",
        "  normalized_doc = text_normalize(doc)\n",
        "  # Tiến hành tách từ\n",
        "  tokens = word_tokenize(normalized_doc)\n",
        "  # Lọc stopwords\n",
        "  non_stopword_tokens = []\n",
        "  for token in tokens:\n",
        "    if token not in stopwords:\n",
        "      non_stopword_tokens.append(token)\n",
        "  # Tiến hành kết hợp các từ ghép trong tiếng Việt bằng '_'\n",
        "  combined_tokens = [token.replace(' ', '_') for token in non_stopword_tokens]\n",
        "  return (normalized_doc, combined_tokens)\n",
        "\n",
        "# Viết hàm lấy danh sách các văn bản/tài liệu thuộc các chủ đề khác nhau\n",
        "def fetch_doc_by_topic(topic, limit = 10):\n",
        "  data_root_dir_path = '/content/IRS/data/vnexpress/{}'.format(topic)\n",
        "  docs = []\n",
        "  doc_count = 0\n",
        "  for file_name in os.listdir(data_root_dir_path):\n",
        "    if doc_count >= limit:\n",
        "      break\n",
        "    file_path = os.path.join(data_root_dir_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "      lines = []\n",
        "      for line in f:\n",
        "        line = line.lower().strip()\n",
        "        lines.append(line)\n",
        "    doc = \" \".join(lines)\n",
        "    clean_doc = re.sub('\\W+',' ', doc)\n",
        "    (normalized_doc, tokens) = preprocess(clean_doc)\n",
        "    docs.append((topic, normalized_doc, tokens))\n",
        "    doc_count+=1\n",
        "  return docs"
      ],
      "metadata": {
        "id": "oKmmm_hQ1bgL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành tạo tập dữ liệu thử nghiệm với các tài liệu/văn bản thuộc danh sách chủ đề [topics] đã lựa chọn bên trên**"
      ],
      "metadata": {
        "id": "TdCueZIW1rVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu trúc dữ liệu dictionary để lưu thông tin chủ đề-tài liệu, nhằm hỗ trợ cho việc tìm kiếm nhanh\n",
        "topic_doc_idxes_dict = {}\n",
        "doc_idx_topic_dict = {}\n",
        "\n",
        "# Duyệt qua từng chủ đề\n",
        "doc_idx = 0\n",
        "for topic in topics:\n",
        "  current_topic_docs = fetch_doc_by_topic(topic, limit_doc_per_topic)\n",
        "  topic_doc_idxes_dict[topic] = []\n",
        "  for (topic, normalized_doc, tokens) in current_topic_docs:\n",
        "    topic_doc_idxes_dict[topic].append(doc_idx)\n",
        "    doc_idx_topic_dict[doc_idx] = topic\n",
        "    doc_idx+=1\n",
        "  D += current_topic_docs\n",
        "\n",
        "doc_size = len(D)\n",
        "\n",
        "print('Hoàn tất, tổng số lượng tài liệu/văn bản đã lấy: [{}]'.format(doc_size))\n",
        "for topic in topic_doc_idxes_dict.keys():\n",
        "  print(' - Chủ đề [{}] có [{}] tài liệu/văn bản.'.format(topic, len(topic_doc_idxes_dict[topic])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hOoa9xu1vaD",
        "outputId": "c1cff0cc-3294-4dc0-86c6-c9c036b51b22"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hoàn tất, tổng số lượng tài liệu/văn bản đã lấy: [15]\n",
            " - Chủ đề [the-thao] có [5] tài liệu/văn bản.\n",
            " - Chủ đề [du-lich] có [5] tài liệu/văn bản.\n",
            " - Chủ đề [khoa-hoc] có [5] tài liệu/văn bản.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành biến đổi các tài liệu/văn bản trong tập (D) về dạng các TF-IDF vectors - trong bài thực hành này chúng ta sẽ sử dụng thư viện Scikit-Learn (TfidfVectorizer) https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html**"
      ],
      "metadata": {
        "id": "FwpTnsEK1yYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo đối tượng TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Chúng ta sẽ tạo ra một tập danh sách các tài liệu/văn bản dạng list đơn giản để thư viện Scikit-Learn có thể đọc được\n",
        "sk_docs = []\n",
        "\n",
        "# Duyệt qua từng tài liệu/văn bản có trong (D)\n",
        "for (topic, normalized_doc, tokens) in D:\n",
        "  # Chúng ta sẽ nối các từ/tokens đã được tách để làm thành một văn bản hoàn chỉnh\n",
        "  text = ' '.join(tokens)\n",
        "  sk_docs.append(text)\n",
        "\n",
        "# Tiến hành chuyển đổi các tài liệu/văn bản về dạng các TF-IDF vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(sk_docs)\n",
        "print(f'Kích thước ma trận tf-idf: [{tfidf_matrix.shape}]')\n",
        "\n",
        "# Chuyển ma trận tfidf_matrix từ dạng cấu trúc thưa sang dạng đầy đủ để thuận tiện cho việc tính toán\n",
        "tfidf_matrix = tfidf_matrix.todense().tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icK715IW1zXL",
        "outputId": "63474db0-845d-4ff2-b9c6-205009795226"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kích thước ma trận tf-idf: [(15, 1843)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành chuẩn bị tập dữ liệu cho bài toán gom cụm với thuật toán Hierarchical Agglomerative Clustering (HAC), với đầu vào sẽ là danh sách các tài liệu/văn bản đã được chuyển đổi về dạng các vector TF-IDF**"
      ],
      "metadata": {
        "id": "w6JNKRXe2B3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo một danh sách chứa thông tin về mã định danh tài liệu (doc_idx) và tf-idf vector tương ứng của nó\n",
        "docs = []\n",
        "for doc_idx, doc_tfidf_vector in enumerate(tfidf_matrix):\n",
        "  docs.append((doc_idx, doc_tfidf_vector))"
      ],
      "metadata": {
        "id": "MiND3EZZ2RgR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành viết các hàm tính độ đo tương đồng và các hướng tiếp cận: liên kết đơn (single link), liên kết hoàn chỉnh (complete link) và trung bình nhóm (group average)**\n",
        "\n",
        "**Trong đó việc xác định mức độ tương đồng giữa hai cụm ($c_{i}$) và ($c_{j}$) theo các hướng tiếp cận:**\n",
        "*   **Liên kết đơn (single link):** \n",
        "## $$sim(c_{i},c_{j}=\\max_{x∈c_{i},y∈c_{j}} sim(x,y)$$\n",
        "*   **Liên kết hoàn chỉnh (complete link):**\n",
        "## $$sim(c_{i},c_{j}=\\min_{x∈c_{i},y∈c_{j}} sim(x,y)$$\n",
        "*   **Trung bình nhóm (group average):**\n",
        "### $$sim(c_{i},c_{j})=\\frac{1}{|c_{i}∪c_{j}|(|c_{i}∪c_{j}|-1)}\\sum_{x∈c_{i}∪c_{j}}\\sum_{y∈c_{i}∪c_{j}:x \\neq y}sim(x,y)$$\n",
        "\n"
      ],
      "metadata": {
        "id": "DRx5-ySmf28j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "enz80BwufVR9"
      },
      "outputs": [],
      "source": [
        "# Tính khoảng cách Euclid giữa vectors biểu diễn tài liệu/văn bản (a) và (b)\n",
        "def euclid_distance(a, b):\n",
        "    return distance.euclidean(a, b)\n",
        "\n",
        "# Tính mức độ tương đồng giữa vectors biểu diễn tài liệu/văn bản (a) và (b)\n",
        "def cosine_sim(a, b):\n",
        "  return 1 - distance.cosine(a, b)\n",
        "\n",
        "# Tính mức độ tương đồng lớn nhất của hai tài liệu/văn bản trong hai cụm (ci) và (cj) \n",
        "def single_link(ci, cj):\n",
        "    # Vì áp dụng khoảng cách Euclid nên chúng ta sẽ lấy khoảng cách nhỏ nhất giữa hai tài liệu/văn bản có trong hai cụm (ci) và (cj) - min()\n",
        "    # Ngược lại nếu dùng tương đồng cosine thì ta sẽ lấy giá trị lớn nhất - max()\n",
        "    return min([euclid_distance(vi[1], vj[1]) for vi in ci for vj in cj])\n",
        "\n",
        "# Tính mức độ tương đồng nhỏ nhất của hai tài liệu/văn bản trong hai cụm (ci) và (cj) \n",
        "def complete_link(ci, cj):\n",
        "    # Tương tự như với tiến cận single link chúng ta sẽ lấy khoảng cách lớn nhất của hai tài liệu/văn bản có trong hai cụm (ci) và (cj) - max()\n",
        "    # Ngược lại nếu dùng tương đồng cosine thì ta sẽ lấy giá trị lớn nhất - min()\n",
        "    return max([euclid_distance(vi[1], vj[1]) for vi in ci for vj in cj])\n",
        "\n",
        "# Tính trung bình khoảng cách giữa tất cả các cặp tài liệu/văn bản trong hai cụm (ci) và (cj)\n",
        "def group_average(ci, cj):\n",
        "    distances = [euclid_distance(vi[1], vj[1]) for vi in ci for vj in cj]\n",
        "    return sum(distances) / len(distances)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cài đặt thuật toán Hierarchical Agglomerative Clustering (HAC)**"
      ],
      "metadata": {
        "id": "WZ9dsoosgF9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HAC_Algorithm:\n",
        "    def __init__(self, docs, k):\n",
        "        self.docs = docs\n",
        "        self.N = len(docs)\n",
        "        self.k = k\n",
        "        # Chúng ta có thể thử qua nhiều cách tiếp cận khác nhau: single link, complete link và group average\n",
        "        self.measure = single_link\n",
        "        self.clusters = self.init_clusters()\n",
        "\n",
        "    # Hàm khởi tạo danh sách các cụm ban đầu\n",
        "    def init_clusters(self):\n",
        "         # Ban đầu thì mỗi tài liệu/văn bản sẽ được gán vào 1 cụm riêng biệt\n",
        "        return {cluster_idx: [(doc_idx, doc_tfidf_vector)] for cluster_idx, (doc_idx, doc_tfidf_vector) in enumerate(self.docs)}\n",
        "\n",
        "    # Hàm tìm cặp cụm (ci) và (cj) gần nhau nhất\n",
        "    # Dựa trên các hướng tiếp cận: single link, complete link và group average - được định nghịa trong hàm measure\n",
        "    def find_closest_clusters(self):\n",
        "        # Ban đầu gán giá trị khoảng cách nhỏ nhất là một số rất lớn - dùng math.inf (vô cực)\n",
        "        min_dist = math.inf\n",
        "        closest_clusters = None\n",
        "        # Lấy ra danh sách các index của các cụm từ [self.clusters]\n",
        "        clusters_ids = list(self.clusters.keys())\n",
        "        \n",
        "        # Lần lượt lặp qua các cặp cụm (ci) và (cj) để lấy ra các cặp cụm gần nhau nhất\n",
        "        for i, cluster_i in enumerate(clusters_ids[:-1]):\n",
        "            for j, cluster_j in enumerate(clusters_ids[i+1:]):\n",
        "                # Tính khoảng cách giữa hai cụm (ci) và (cj)\n",
        "                dist = self.measure(self.clusters[cluster_i], self.clusters[cluster_j])\n",
        "                # Kiểm tra xem khoảng cách này có phải là nhỏ nhất hay không ?\n",
        "                if dist < min_dist:\n",
        "                    # Nếu là khoảng cách nhỏ nhất thì cập nhật lại giá trị [min_dist]\n",
        "                    # Cùng với thông tin cặp cụm (ci) và (cj) gần nhau nhất\n",
        "                    min_dist, closest_clusters = dist, (cluster_i, cluster_j)\n",
        "        return closest_clusters\n",
        "\n",
        "    # Hàm trộn hai cụm (ci) và (cj) để tạo một cụm mới\n",
        "    def merge_and_form_new_clusters(self, ci_id, cj_id):\n",
        "        # Tạo một cụm mới tại ví trí đầu tiên [0] - trong [new_clusters]\n",
        "        # Và dữ liệu của cụm là tập hợp danh sách các tài liệu/văn bản trong 2 cụm (ci) và (cj) gộp lại\n",
        "        new_clusters = {0: self.clusters[ci_id] + self.clusters[cj_id]}\n",
        "        # Lặp qua tất cả các cụm hiện tại có trong [self.clusters]\n",
        "        for cluster_id in self.clusters.keys():\n",
        "            # Nếu là hai cụm cũ (ci) và (cj) đang xét thì bỏ qua\n",
        "            if (cluster_id == ci_id) | (cluster_id == cj_id):\n",
        "                continue\n",
        "            # Tiến hành bỏ thông tin các cụm cũ vào [new_clusters] với key tăng dần lên: 1, 2, 3, ...\n",
        "            new_clusters[len(new_clusters.keys())] = self.clusters[cluster_id]\n",
        "        return new_clusters\n",
        "\n",
        "    # Hàm chính chạy thuật toán\n",
        "    def run(self):\n",
        "        # Kiểm tra xem số lượng cụm trong [self.clusters] đã đúng với số lượng cụm (k) kỳ vọng chưa\n",
        "        # Nếu số lượng cụm trong [self.clusters] vẫn lớn hơn thì tiếp tục thực hiện gộp\n",
        "        while len(self.clusters.keys()) > self.k:\n",
        "            # Tiến hành tìm kiếm và trộn hai cụm (ci) và (cj) có khoảng cách gần nhau/độ tương đồng cao nhất \n",
        "            closest_clusters = self.find_closest_clusters()\n",
        "            # Tiến hành trộn hai cụm (ci) và (cj) gần nhau nhất thành 1 cụm & cập nhật lại dữ liệu các cụm\n",
        "            self.clusters = self.merge_and_form_new_clusters(*closest_clusters)\n",
        "\n",
        "    # Xuất thông tin các cụm sau khi hoàn tất\n",
        "    def show_cluster_info(self):\n",
        "        for cluster_idx, cluster_data in self.clusters.items():\n",
        "            print(\"Cụm (cluster) [{}]:\".format(cluster_idx))\n",
        "            for (doc_idx, doc_tfidf_vector) in cluster_data:\n",
        "                print(f\" - Chủ đề: [{doc_idx_topic_dict[doc_idx]}]/mã tài liệu: [{doc_idx}]\")"
      ],
      "metadata": {
        "id": "EDPgVoKRgS0-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành chạy thuật toán HAC với tập dữ liệu thuộc các chủ đề khác trong nhau tập [docs] - chúng ta sẽ lấy giá trị số lượng cụm kỳ vọng bằng với số lượng chủ đề - ví dụ: k = 3**"
      ],
      "metadata": {
        "id": "ugEcgEw05Gmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khai báo số lượng cụm kỳ vọng (k)\n",
        "k = number_of_topics\n",
        "\n",
        "# Khởi tạo & khai báo các tham số cho mô hình gom cụm HAC \n",
        "hac_algorithm = HAC_Algorithm(docs, k)\n",
        "\n",
        "# Tiến hành chạy thuật toán HAC\n",
        "print(f'Tiến hành chạy thuật toán HAC với tập dữ liệu: [{len(docs)}] tài liệu/văn bản - số cụm kỳ vọng (k) = [{k}]...')\n",
        "hac_algorithm.run()\n",
        "\n",
        "# Hoàn tất - xuất kết quả thông tin các cụm\n",
        "print('Hoàn tất ! xuất thông tin các cụm...')\n",
        "hac_algorithm.show_cluster_info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7UFFJVehDhh",
        "outputId": "e3f83984-3a19-48f3-8248-476c0f444cd5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiến hành chạy thuật toán HAC với tập dữ liệu: [15] tài liệu/văn bản - số cụm kỳ vọng (k) = [3]...\n",
            "Hoàn tất ! xuất thông tin các cụm...\n",
            "Cụm (cluster) [0]:\n",
            " - Chủ đề: [the-thao]/mã tài liệu: [0]\n",
            " - Chủ đề: [the-thao]/mã tài liệu: [4]\n",
            " - Chủ đề: [the-thao]/mã tài liệu: [2]\n",
            " - Chủ đề: [the-thao]/mã tài liệu: [1]\n",
            " - Chủ đề: [the-thao]/mã tài liệu: [3]\n",
            " - Chủ đề: [khoa-hoc]/mã tài liệu: [10]\n",
            " - Chủ đề: [khoa-hoc]/mã tài liệu: [12]\n",
            " - Chủ đề: [du-lich]/mã tài liệu: [8]\n",
            " - Chủ đề: [du-lich]/mã tài liệu: [6]\n",
            " - Chủ đề: [du-lich]/mã tài liệu: [5]\n",
            " - Chủ đề: [du-lich]/mã tài liệu: [7]\n",
            " - Chủ đề: [khoa-hoc]/mã tài liệu: [13]\n",
            " - Chủ đề: [khoa-hoc]/mã tài liệu: [11]\n",
            "Cụm (cluster) [1]:\n",
            " - Chủ đề: [du-lich]/mã tài liệu: [9]\n",
            "Cụm (cluster) [2]:\n",
            " - Chủ đề: [khoa-hoc]/mã tài liệu: [14]\n"
          ]
        }
      ]
    }
  ]
}