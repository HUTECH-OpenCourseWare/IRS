{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import các thư viện cần thiết**"
      ],
      "metadata": {
        "id": "6FkGBoya3WLK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DgUJG3RA3NNf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.spatial import distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tập dữ liệu văn bản/tài liệu mẫu và truy vấn**"
      ],
      "metadata": {
        "id": "mCk5ee4t3bCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tập văn bản/tài liệu (D)\n",
        "docs = [\n",
        "    'data mining is awesome. data mining helps to find frequent itemsets in database.',\n",
        "    'information retrieval is cool. information retrieval helps to search data quickly.',\n",
        "    'natural language processing is interesting. it helps computer to better understand text.'\n",
        "];\n",
        "\n",
        "# Truy vấn (q)\n",
        "query = 'information retrieval'\n",
        "\n",
        "# Để cho tiện lợi trong việc xử lý chúng ta sẽ gán câu truy vấn là 1 tài liệu cuối cùng\n",
        "docs.append(query)\n",
        "\n",
        "# Làm sạch văn bản - xóa các ký tự đặc biệt, dấu câu, v.v.\n",
        "docs = [re.sub('\\W+',' ', doc) for doc in docs] \n",
        "\n",
        "doc_len = len(docs)"
      ],
      "metadata": {
        "id": "SAJQgP0R3X_J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Đọc từng tài liệu/văn bản và thực hiện tách từ - gán id cho các token và xây dựng tập từ vựng**\n",
        "\n",
        "**Ngoài ra chúng ta cũng cần xác định thêm các thông tin khác, bao gồm:**\n",
        "\n",
        "*   **Số lần xuất hiện của mỗi token/từ trong mỗi văn bản (term frequency)**\n",
        "*   **Số lượng văn bản mà mỗi token/từ xuất hiện (xét trong toàn tập văn bản)**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-pDJBk6n3jAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu trúc dữ liệu dạng dictionary -> <key: id, value: token>\n",
        "id_token_dict = {}\n",
        "\n",
        "# Cấu trúc dữ liệu dạng dictionary -> <key: token, value: id>\n",
        "token_id_dict = {}\n",
        "\n",
        "# Cấu trúc dữ liệu dạng dictionary -> <key: doc_id, value: [id_token_xuất_hiện_1, id_token_xuất_hiện_2, v.v. ]>\n",
        "doc_id_token_ids_dict = {}\n",
        "\n",
        "# Cấu trúc dữ liệu dạng dictionary -> <key: token_id, value: <key: doc_id, value: token_freq>\n",
        "token_id_doc_id_term_freq_dict = {}\n",
        "\n",
        "# Cấu trúc dữ liệu dạng dictionary -> <key: doc_id, value: <key: token_id, value: token_freq>\n",
        "doc_id_token_id_term_freq_dict = {}\n",
        "\n",
        "# Cấu trúc dữ liệu dạng dictionary -> <key: token_id, value: [doc_id_token_xuất_hiện_1, doc_id_token_xuất_hiện_2, v.v.]>\n",
        "token_id_doc_ids_dict = {}\n",
        "\n",
        "# Gán id của token đầu tiên là [0]\n",
        "token_id = 0\n",
        "\n",
        "# Duyệt qua từng tài liệu/văn bản có trong mảng\n",
        "for doc_idx, doc in enumerate(docs):\n",
        "  # Tách các từ riêng biệt trong tài liệu/văn bản bằng khoảng trắng \" \"\n",
        "  tokens = doc.split(' ')\n",
        "  \n",
        "  # Khởi tạo danh sách các token_ids xuất hiện trong tài liệu/văn bản\n",
        "  doc_token_ids = []\n",
        "\n",
        "  # Đọc qua từng token trong mỗi tài liệu/văn bản\n",
        "  for token in tokens:\n",
        "    # Kiểm tra chiều dài từ khóa > 0\n",
        "    if len(token) > 0:\n",
        "      if token not in token_id_dict.keys():\n",
        "        token_id_dict[token] = token_id\n",
        "        id_token_dict[token_id] = token\n",
        "        # Tăng id của token tiếp theo lên 1\n",
        "        token_id+=1\n",
        "\n",
        "      target_token_id = token_id_dict[token]\n",
        "\n",
        "      # Đếm số lượng mà token này xuất hiện trong doc_idx\n",
        "      if target_token_id not in token_id_doc_id_term_freq_dict.keys():\n",
        "        token_id_doc_id_term_freq_dict[target_token_id] = {}\n",
        "        token_id_doc_id_term_freq_dict[target_token_id][doc_idx] = 1\n",
        "      else:\n",
        "        if doc_idx not in token_id_doc_id_term_freq_dict[target_token_id].keys():\n",
        "          token_id_doc_id_term_freq_dict[target_token_id][doc_idx] = 1\n",
        "        else:\n",
        "          token_id_doc_id_term_freq_dict[target_token_id][doc_idx] += 1\n",
        "      \n",
        "      if doc_idx not in doc_id_token_id_term_freq_dict.keys():\n",
        "        doc_id_token_id_term_freq_dict[doc_idx] = {}\n",
        "        doc_id_token_id_term_freq_dict[doc_idx][target_token_id] = 1\n",
        "      else:\n",
        "        if token_id not in doc_id_token_id_term_freq_dict[doc_idx].keys():\n",
        "          doc_id_token_id_term_freq_dict[doc_idx][target_token_id] = 1\n",
        "        else:\n",
        "          doc_id_token_id_term_freq_dict[doc_idx][target_token_id] += 1\n",
        "\n",
        "      # Gán doc_idx vào danh sách mà token này xuất hiện\n",
        "      if target_token_id not in token_id_doc_ids_dict.keys():\n",
        "        token_id_doc_ids_dict[target_token_id] = [doc_idx]\n",
        "      else:\n",
        "        if doc_idx not in token_id_doc_ids_dict[target_token_id]:\n",
        "          token_id_doc_ids_dict[target_token_id].append(doc_idx)\n",
        "\n",
        "      doc_token_ids.append(token_id_dict[token])\n",
        "  doc_id_token_ids_dict[doc_idx] = doc_token_ids\n",
        "\n",
        "# Xuất ra màn hình kích thước tập từ vựng (vocabulary)\n",
        "vocab_size = len(id_token_dict.keys())\n",
        "print('Kích thước tập từ vựng: [{}]'.format(vocab_size))\n",
        "print('Tập từ vựng (V):')\n",
        "print(id_token_dict)\n",
        "print('---')\n",
        "print('Thông tin về tầng số xuất hiện của từ trong mỗi tài liệu/văn bản:')\n",
        "print(token_id_doc_id_term_freq_dict)\n",
        "print(doc_id_token_id_term_freq_dict)\n",
        "print('---')\n",
        "print('Thông tin về từ - danh sách các tài liệu/văn bản mà nó xuất hiện:')\n",
        "print(token_id_doc_ids_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXCEyD0v3g-A",
        "outputId": "0b88fe37-5194-477c-e37e-1bafb551d09c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kích thước tập từ vựng: [25]\n",
            "Tập từ vựng (V):\n",
            "{0: 'data', 1: 'mining', 2: 'is', 3: 'awesome', 4: 'helps', 5: 'to', 6: 'find', 7: 'frequent', 8: 'itemsets', 9: 'in', 10: 'database', 11: 'information', 12: 'retrieval', 13: 'cool', 14: 'search', 15: 'quickly', 16: 'natural', 17: 'language', 18: 'processing', 19: 'interesting', 20: 'it', 21: 'computer', 22: 'better', 23: 'understand', 24: 'text'}\n",
            "---\n",
            "Thông tin về tầng số xuất hiện của từ trong mỗi tài liệu/văn bản:\n",
            "{0: {0: 2, 1: 1}, 1: {0: 2}, 2: {0: 1, 1: 1, 2: 1}, 3: {0: 1}, 4: {0: 1, 1: 1, 2: 1}, 5: {0: 1, 1: 1, 2: 1}, 6: {0: 1}, 7: {0: 1}, 8: {0: 1}, 9: {0: 1}, 10: {0: 1}, 11: {1: 2, 3: 1}, 12: {1: 2, 3: 1}, 13: {1: 1}, 14: {1: 1}, 15: {1: 1}, 16: {2: 1}, 17: {2: 1}, 18: {2: 1}, 19: {2: 1}, 20: {2: 1}, 21: {2: 1}, 22: {2: 1}, 23: {2: 1}, 24: {2: 1}}\n",
            "{0: {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1}, 1: {11: 1, 12: 1, 2: 1, 13: 1, 4: 1, 5: 1, 14: 1, 0: 1, 15: 1}, 2: {16: 1, 17: 1, 18: 1, 2: 1, 19: 1, 20: 1, 4: 1, 21: 1, 5: 1, 22: 1, 23: 1, 24: 1}, 3: {11: 1, 12: 1}}\n",
            "---\n",
            "Thông tin về từ - danh sách các tài liệu/văn bản mà nó xuất hiện:\n",
            "{0: [0, 1], 1: [0], 2: [0, 1, 2], 3: [0], 4: [0, 1, 2], 5: [0, 1, 2], 6: [0], 7: [0], 8: [0], 9: [0], 10: [0], 11: [1, 3], 12: [1, 3], 13: [1], 14: [1], 15: [1], 16: [2], 17: [2], 18: [2], 19: [2], 20: [2], 21: [2], 22: [2], 23: [2], 24: [2]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thực hiện tính TF-IDF và chuyển đổi các tài liệu/văn bản về dạng TF-IDF vector**\n",
        "\n",
        "**Để xác định được TF-IDF của một từ (i) cho một văn bản (j), chúng ta cần xác định ($tf_{ij}$) và ($idf_{i}$), với:**\n",
        "$$tf_{ij}=\\frac{f_{ij}}{max_i(f_{ij})}$$\n",
        "$$idf_{i}=log_{2}(\\frac{n}{df_{i}})$$\n",
        "**Trong đó**,\n",
        "*   ($max(f_{ij})$): là tầng số xuất hiện cao nhất của một từ trong văn bản (j)\n",
        "*   (n): là tổng số lượng tài liệu/văn bản [doc_len]\n",
        "*   ($df_{i}$): là số lượng tài liệu/văn bản có từ khóa/token (i) xuất hiện\n",
        "\n"
      ],
      "metadata": {
        "id": "A-j39koW8KIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Viết hàm xác định từ/token xuất hiện nhiều nhất trong 1 văn bản (d) và số lần xuất hiện\n",
        "def find_max_freq_token(doc_idx):\n",
        "  max_token_id = -1\n",
        "  max_freq = 0\n",
        "  for token_id in doc_id_token_id_term_freq_dict[doc_idx].keys():\n",
        "    if doc_id_token_id_term_freq_dict[doc_idx][token_id] > max_freq:\n",
        "      max_token_id = token_id\n",
        "      max_freq = doc_id_token_id_term_freq_dict[doc_idx][token_id]\n",
        "  return (max_token_id, max_freq)\n",
        "\n",
        "# Viết hàm tính trọng số TF-IDF của 1 từ/token (t) và 1 văn bản (d)\n",
        "def calc_tfidf(token_id, doc_idx):\n",
        "  (max_token_id, max_freq) = find_max_freq_token(doc_idx)\n",
        "  tf = token_id_doc_id_term_freq_dict[token_id][doc_idx] / max_freq\n",
        "  df = len(token_id_doc_ids_dict[token_id])\n",
        "  idf = math.log((doc_len / df), 2)\n",
        "  return tf * idf\n",
        "\n",
        "# Cấu trúc dữ liệu dạng dictionary -> <key: doc_id, value: tfidf_encoded_vector>\n",
        "doc_id_tfidf_encoded_vector_dict = {}\n",
        "\n",
        "# Duyệt qua từng tài liệu/văn bản có trong mảng\n",
        "for doc_idx in doc_id_token_ids_dict.keys():\n",
        "  # Khởi tạo một vector có số chiều = vocab_size và các giá trị [0] -> [0, 0, 0, ...]\n",
        "  tfidf_encoded_vector = np.zeros(vocab_size)\n",
        "\n",
        "  # Danh sách các token_ids xuất hiện trong tài liệu/văn bản\n",
        "  doc_token_ids = doc_id_token_ids_dict[doc_idx]\n",
        "  \n",
        "  # Duyệt qua từng token xuất hiện trong tài liệu/văn bản\n",
        "  for token_id in doc_token_ids:\n",
        "    # Gán vị trí - cột/chiều thứ (i) của vector tính giá trị TF-IDF - tương đương với vị trí xuất hiện của token\n",
        "    tfidf_encoded_vector[token_id] = calc_tfidf(token_id, doc_idx)\n",
        "  doc_id_tfidf_encoded_vector_dict[doc_idx] = tfidf_encoded_vector\n",
        "\n",
        "# TFIDF encode của truy vấn (q) là tài liệu cuối cùng\n",
        "query_tfidf_encoded_vector = doc_id_tfidf_encoded_vector_dict[doc_len-1]\n",
        "\n",
        "# Xóa query đã được mã hóa thành dạng tfidf vector ra khỏi danh sách\n",
        "del doc_id_tfidf_encoded_vector_dict[doc_len-1]\n",
        "\n",
        "print('Các tài liệu được chuyển đổi về dạng tf-idf vectors:')\n",
        "for doc_idx in doc_id_tfidf_encoded_vector_dict.keys():\n",
        "  doc_tfidf_encoded_vector = doc_id_tfidf_encoded_vector_dict[doc_idx]\n",
        "  print(doc_tfidf_encoded_vector)\n",
        "\n",
        "print('Truy vấn được chuyển đổi về dạng tf-idf vector:')\n",
        "print(query_tfidf_encoded_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58z4qWUW8RIo",
        "outputId": "c8a9b1a8-9bbe-4d7d-bce5-78e1d6ef7478"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Các tài liệu được chuyển đổi về dạng tf-idf vectors:\n",
            "[2.        4.        0.4150375 2.        0.4150375 0.4150375 2.\n",
            " 2.        2.        2.        2.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.       ]\n",
            "[1.        0.        0.4150375 0.        0.4150375 0.4150375 0.\n",
            " 0.        0.        0.        0.        2.        2.        2.\n",
            " 2.        2.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.       ]\n",
            "[0.        0.        0.4150375 0.        0.4150375 0.4150375 0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        2.        2.        2.        2.        2.\n",
            " 2.        2.        2.        2.       ]\n",
            "Truy vấn được chuyển đổi về dạng tf-idf vector:\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tìm danh sách các tài liệu liên quan đến truy vấn (Tích vô hướng, khoảng cách Euclid và Tương đồng cosine)**"
      ],
      "metadata": {
        "id": "3aLwNLZXGLL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Duyệt qua danh sách các tài liệu/văn bản (đã mã hóa ở dạng one-hot vectors)\n",
        "for doc_idx in doc_id_tfidf_encoded_vector_dict.keys():\n",
        "  doc_tfidf_encoded_vector = doc_id_tfidf_encoded_vector_dict[doc_idx]\n",
        "  \n",
        "  # Tính tích vô hướng giữa hai vectors tài liệu và truy vấn\n",
        "  dot_product_sim = np.dot(query_tfidf_encoded_vector, doc_tfidf_encoded_vector)\n",
        "\n",
        "  # Tính khoảng cách Euclid giữa hai vectors tài liệu và truy vấn\n",
        "  ed = distance.euclidean(query_tfidf_encoded_vector, doc_tfidf_encoded_vector)\n",
        "  \n",
        "  # Tính tương đồ cosine giữa hai vectors tài liệu và truy vấn\n",
        "  cs = 1 - distance.cosine(query_tfidf_encoded_vector, doc_tfidf_encoded_vector)\n",
        "\n",
        "  print('Tài liệu: [{}], tương đồng (dot product): [{:.6f}]'.format(doc_idx, dot_product_sim))\n",
        "  print('Tài liệu: [{}], tương đồng (khoảng cách Euclid): [{:.6f}]'.format(doc_idx, ed))\n",
        "  print('Tài liệu: [{}], tương đồng (Tương đồng cosine): [{:.6f}]'.format(doc_idx, cs))\n",
        "  print('---')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8He1ETetGP25",
        "outputId": "380bb4ae-6324-440e-9b05-5052b43547da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tài liệu: [0], tương đồng (dot product): [0.000000]\n",
            "Tài liệu: [0], tương đồng (khoảng cách Euclid): [6.820320]\n",
            "Tài liệu: [0], tương đồng (Tương đồng cosine): [0.000000]\n",
            "---\n",
            "Tài liệu: [1], tương đồng (dot product): [4.000000]\n",
            "Tài liệu: [1], tương đồng (khoảng cách Euclid): [3.939133]\n",
            "Tài liệu: [1], tương đồng (Tương đồng cosine): [0.609757]\n",
            "---\n",
            "Tài liệu: [2], tương đồng (dot product): [0.000000]\n",
            "Tài liệu: [2], tương đồng (khoảng cách Euclid): [6.206188]\n",
            "Tài liệu: [2], tương đồng (Tương đồng cosine): [0.000000]\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}