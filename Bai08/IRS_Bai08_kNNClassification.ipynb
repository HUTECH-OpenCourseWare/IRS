{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Cài đặt thư viện UnderTheSea để hỗ trợ tách từ trong tiếng Việt. Xem thêm các tính năng của thư viện tại: https://github.com/undertheseanlp/underthesea**"
      ],
      "metadata": {
        "id": "n4jorOXVsgM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0YqIYQDsiJu",
        "outputId": "f605b0cf-5db3-44e7-b2a7-d02e3ceadda5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting underthesea\n",
            "  Downloading underthesea-1.4.0-py3-none-any.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.8/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from underthesea) (4.64.1)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from underthesea) (6.0)\n",
            "Collecting underthesea-core==0.0.5a2\n",
            "  Downloading underthesea_core-0.0.5_alpha.2-cp38-cp38-manylinux2010_x86_64.whl (591 kB)\n",
            "\u001b[K     |████████████████████████████████| 591 kB 32.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from underthesea) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from underthesea) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from underthesea) (1.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->underthesea) (2022.6.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (1.7.3)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.8 underthesea-1.4.0 underthesea-core-0.0.5a2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import các thư viện cần thiết**"
      ],
      "metadata": {
        "id": "CGp_zzhbqytp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aJbu9MCiqtPp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from underthesea import text_normalize\n",
        "from underthesea import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.spatial import distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lấy dữ liệu các tài liệu/văn bản mẫu từ**: https://github.com/HUTECH-OpenCourseWare/IRS.git"
      ],
      "metadata": {
        "id": "ZG64el0_r2T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HUTECH-OpenCourseWare/IRS.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t5LaRU-r25A",
        "outputId": "996ab97d-f7e3-42a9-ca4f-4769bc1ec432"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IRS'...\n",
            "remote: Enumerating objects: 271, done.\u001b[K\n",
            "remote: Counting objects: 100% (271/271), done.\u001b[K\n",
            "remote: Compressing objects: 100% (271/271), done.\u001b[K\n",
            "remote: Total 271 (delta 0), reused 271 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (271/271), 441.48 KiB | 1.74 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành thử nghiệm với danh sách các tài liệu/văn bản thuộc các chủ đề khác nhau**"
      ],
      "metadata": {
        "id": "VnR6NrY3r8af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chọn danh sách các chủ đề của tài liệu/văn bản cho thử nghiệm\n",
        "topics = [\n",
        "    'the-thao',\n",
        "    'giao-duc',\n",
        "    'khoa-hoc'\n",
        "]\n",
        "\n",
        "# Tạo một tập dữ liệu thử nghiệm gồm các tài liệu/văn bản thuộc về 2-3 chủ đề\n",
        "# Cấu trúc dữ liệu dạng list - lưu thông tin danh sách các tài liệu/văn bản thuộc chủ đề khác nhau\n",
        "# Mỗi tài liệu/văn bản sẽ tổ chức dạng 1 tuple với: (topic, nội_dung_văn_bản, danh_sách_token)\n",
        "D = []"
      ],
      "metadata": {
        "id": "qEh2Spr4r5z7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành viết một số hàm hỗ trợ cho việc đọc dữ liệu, xử lý và tách từ trong tiếng Việt.**"
      ],
      "metadata": {
        "id": "_8U0i3aGsIPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Viết hàm tiền xử lý và tách từ tiếng Việt\n",
        "def preprocess(doc):\n",
        "  # Tiến hành xử lý các lỗi từ/câu, dấu câu, v.v. trong tiếng Việt với hàm text_normalize\n",
        "  normalized_doc = text_normalize(doc)\n",
        "  # Tiến hành tách từ\n",
        "  tokens = word_tokenize(normalized_doc)\n",
        "  # Tiến hành kết hợp các từ ghép trong tiếng Việt bằng '_'\n",
        "  combined_tokens = [token.replace(' ', '_') for token in tokens]\n",
        "  return (normalized_doc, combined_tokens)\n",
        "\n",
        "# Viết hàm lấy danh sách các văn bản/tài liệu thuộc các chủ đề khác nhau\n",
        "def fetch_doc_by_topic(topic):\n",
        "  data_root_dir_path = '/content/IRS/data/vnexpress/{}'.format(topic)\n",
        "  docs = []\n",
        "  for file_name in os.listdir(data_root_dir_path):\n",
        "    file_path = os.path.join(data_root_dir_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "      lines = []\n",
        "      for line in f:\n",
        "        line = line.lower().strip()\n",
        "        lines.append(line)\n",
        "    doc = \" \".join(lines)\n",
        "    clean_doc = re.sub('\\W+',' ', doc)\n",
        "    (normalized_doc, tokens) = preprocess(clean_doc)\n",
        "    docs.append((topic, normalized_doc, tokens))\n",
        "  return docs"
      ],
      "metadata": {
        "id": "gHT1G7dKsLwk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành tạo tập dữ liệu thử nghiệm với các tài liệu/văn bản thuộc danh sách chủ đề [topics] đã lựa chọn bên trên**"
      ],
      "metadata": {
        "id": "DPbmvZv2sQGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu trúc dữ liệu dictionary để lưu thông tin chủ đề-tài liệu, nhằm hỗ trợ cho việc tìm kiếm nhanh\n",
        "topic_doc_idxes_dict = {}\n",
        "doc_idx_topic_dict = {}\n",
        "\n",
        "# Duyệt qua từng chủ đề\n",
        "doc_idx = 0\n",
        "for topic in topics:\n",
        "  current_topic_docs = fetch_doc_by_topic(topic)\n",
        "  topic_doc_idxes_dict[topic] = []\n",
        "  for (topic, normalized_doc, tokens) in current_topic_docs:\n",
        "    topic_doc_idxes_dict[topic].append(doc_idx)\n",
        "    doc_idx_topic_dict[doc_idx] = topic\n",
        "    doc_idx+=1\n",
        "  D += current_topic_docs\n",
        "\n",
        "doc_size = len(D)\n",
        "\n",
        "print('Hoàn tất, tổng số lượng tài liệu/văn bản đã lấy: [{}]'.format(doc_size))\n",
        "for topic in topic_doc_idxes_dict.keys():\n",
        "  print(' - Chủ đề [{}] có [{}] tài liệu/văn bản.'.format(topic, len(topic_doc_idxes_dict[topic])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xH9jCaRsR2s",
        "outputId": "f2c9e08b-2b7d-4150-d53d-f4b843d785a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hoàn tất, tổng số lượng tài liệu/văn bản đã lấy: [109]\n",
            " - Chủ đề [the-thao] có [39] tài liệu/văn bản.\n",
            " - Chủ đề [giao-duc] có [35] tài liệu/văn bản.\n",
            " - Chủ đề [khoa-hoc] có [35] tài liệu/văn bản.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành biến đổi các tài liệu/văn bản trong tập (D) về dạng các TF-IDF vectors - trong bài thực hành này chúng ta sẽ sử dụng thư viện Scikit-Learn (TfidfVectorizer) https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html**"
      ],
      "metadata": {
        "id": "Suyhqc2Pswy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo đối tượng TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Chúng ta sẽ tạo ra một tập danh sách các tài liệu/văn bản dạng list đơn giản để thư viện Scikit-Learn có thể đọc được\n",
        "sk_docs = []\n",
        "\n",
        "# Duyệt qua từng tài liệu/văn bản có trong (D)\n",
        "for (topic, normalized_doc, tokens) in D:\n",
        "  # Chúng ta sẽ nối các từ/tokens đã được tách để làm thành một văn bản hoàn chỉnh\n",
        "  text = ' '.join(tokens)\n",
        "  sk_docs.append(text)\n",
        "\n",
        "# Tiến hành chuyển đổi các tài liệu/văn bản về dạng các TF-IDF vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(sk_docs)\n",
        "\n",
        "# Chuyển ma trận tfidf_matrix từ dạng cấu trúc thưa sang dạng đầy đủ để thuận tiện cho việc tính toán\n",
        "tfidf_matrix = tfidf_matrix.todense()"
      ],
      "metadata": {
        "id": "2abfe7lCsxik"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QUÁ TRÌNH PHÂN LỚP: với phương pháp phân lớp dựa trên kNN chúng ta sẽ không cần đến quá trình huấn luyện mô hình mà sẽ trực tiếp dùng tập dữ liệu huấn luyện để xác định chủ đề/lớp cho văn bản kiểm thử (mới)**"
      ],
      "metadata": {
        "id": "RXVeieYEtYDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Xác định số lượng hàng xóm lân cận (k) - giá trị (k) nên là số lẻ\n",
        "k = 5\n",
        "\n",
        "# Viết hàm giúp chuyển đổi một văn bản mới về dạng tfidf vector\n",
        "def parse_text(text):\n",
        "  (normalized_doc, combined_tokens) = preprocess(text)\n",
        "  parsed_text = ' '.join(combined_tokens)\n",
        "  text_tfidf_vector = vectorizer.transform([parsed_text])[0].todense()\n",
        "  return text_tfidf_vector\n",
        "\n",
        "test_doc = 'Trừ La Liga, bốn giải lớn còn lại của châu Âu đều thi đấu cuối tuần này trước khi các CLB nhả người cho đội tuyển dịp World Cup 2022. Man City đóng góp năm cầu thủ trong danh sách tuyển Anh dự World Cup 2022, nhiều nhất trong số các CLB Ngoại hạng Anh. Dựa vào những gì các học trò Pep Guardiola đang thể hiện, đó không phải bất ngờ. Nhưng HLV Brentford Thomas Frank cho rằng học trò của ông, tiền đạo Ivan Toney, xứng đáng không kém.'\n",
        "\n",
        "test_doc_tfidf_vector = parse_text(test_doc)\n",
        "\n",
        "# Chúng ta có một cấu trúc dictionary để lưu trữ mức độ tương đồng \n",
        "# giữa tài liệu/văn bản kiểm thử & các tài liệu/văn bản trong tập huấn luyện\n",
        "doc_idx_test_doc_sim_dict = {}\n",
        "\n",
        "print('Xác định mức độ tương đồng giữa tập tài liệu/văn bản huấn luyện và tài liệu/văn bản kiểm thử...')\n",
        "for doc_idx, doc_tfidf_vector in enumerate(tfidf_matrix):\n",
        "  cs_sim = 1 - distance.cosine(doc_tfidf_vector, test_doc_tfidf_vector)\n",
        "  doc_idx_test_doc_sim_dict[doc_idx] = cs_sim\n",
        "\n",
        "# Sắp xếp lại thứ tự các tài liệu/văn bản theo mức độ tương đồng (giảm dần)\n",
        "sorted_doc_idx_test_doc_sim_dict = sorted(doc_idx_test_doc_sim_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "# Lấy ra top-k tài liệu/văn bản gần nhất với tài liệu/văn bản kiểm thử [test_doc_tfidf_vector]\n",
        "top_k_nearest_neighors = sorted_doc_idx_test_doc_sim_dict[:k]\n",
        "\n",
        "# Chúng ta có một dictionary để thống kê số lượng các chủ đề của các tài liệu/văn bản gần nhất với [test_doc_tfidf_vector]\n",
        "top_k_topic_dict = {}\n",
        "\n",
        "for (doc_idx, cs_sim) in top_k_nearest_neighors:\n",
        "  belonged_topic = doc_idx_topic_dict[doc_idx]\n",
        "  print(f' - Tài liệu: [{doc_idx}] - chủ đề: [{belonged_topic}], có độ tương đồng: [{cs_sim:.6f}]')\n",
        "  if belonged_topic not in top_k_topic_dict.keys():\n",
        "    top_k_topic_dict[belonged_topic] = 1\n",
        "  else:\n",
        "    top_k_topic_dict[belonged_topic] += 1\n",
        "\n",
        "# Sắp xếp lại theo thứ tự giảm dần của số lượng chủ đề xuất hiện\n",
        "sorted_top_k_topic_dict = sorted(top_k_topic_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "# Xác định chủ đề/lớp của [test_doc_tfidf_vector] chính là chủ đề/lớp xuất hiện nhiều nhất\n",
        "print(f'Chủ đề/lớp của tài liệu/văn bản được xác định là: [{sorted_top_k_topic_dict[0][0]}]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkxoSHdut6pu",
        "outputId": "2e52155a-7d08-4da8-9428-99770ce3acef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xác định mức độ tương đồng giữa tập tài liệu/văn bản huấn luyện và tài liệu/văn bản kiểm thử...\n",
            " - Tài liệu: [5] - chủ đề: [the-thao], có độ tương đồng: [0.226069]\n",
            " - Tài liệu: [1] - chủ đề: [the-thao], có độ tương đồng: [0.185795]\n",
            " - Tài liệu: [11] - chủ đề: [the-thao], có độ tương đồng: [0.184284]\n",
            " - Tài liệu: [29] - chủ đề: [the-thao], có độ tương đồng: [0.184202]\n",
            " - Tài liệu: [15] - chủ đề: [the-thao], có độ tương đồng: [0.158986]\n",
            "Chủ đề/lớp của tài liệu/văn bản được xác định là: [the-thao]\n"
          ]
        }
      ]
    }
  ]
}