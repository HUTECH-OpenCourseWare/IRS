{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Cài đặt thư viện UnderTheSea để hỗ trợ tách từ trong tiếng Việt. Xem thêm các tính năng của thư viện tại: https://github.com/undertheseanlp/underthesea**"
      ],
      "metadata": {
        "id": "n4jorOXVsgM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0YqIYQDsiJu",
        "outputId": "36b806a5-de31-4189-ea98-811c86ac1c60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting underthesea\n",
            "  Downloading underthesea-1.4.0-py3-none-any.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from underthesea) (2.23.0)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 56.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from underthesea) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from underthesea) (4.64.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from underthesea) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from underthesea) (1.2.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.8/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from underthesea) (6.0)\n",
            "Collecting underthesea-core==0.0.5a2\n",
            "  Downloading underthesea_core-0.0.5_alpha.2-cp38-cp38-manylinux2010_x86_64.whl (591 kB)\n",
            "\u001b[K     |████████████████████████████████| 591 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->underthesea) (2022.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (1.21.6)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.8 underthesea-1.4.0 underthesea-core-0.0.5a2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import các thư viện cần thiết**"
      ],
      "metadata": {
        "id": "CGp_zzhbqytp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aJbu9MCiqtPp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from underthesea import text_normalize\n",
        "from underthesea import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.spatial import distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lấy dữ liệu các tài liệu/văn bản mẫu từ**: https://github.com/HUTECH-OpenCourseWare/IRS.git"
      ],
      "metadata": {
        "id": "ZG64el0_r2T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HUTECH-OpenCourseWare/IRS.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t5LaRU-r25A",
        "outputId": "f85bd7ba-f862-425e-fb1d-43baa5f070c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IRS'...\n",
            "remote: Enumerating objects: 271, done.\u001b[K\n",
            "remote: Counting objects: 100% (271/271), done.\u001b[K\n",
            "remote: Compressing objects: 100% (271/271), done.\u001b[K\n",
            "remote: Total 271 (delta 0), reused 271 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (271/271), 441.48 KiB | 1.81 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành thử nghiệm với danh sách các tài liệu/văn bản thuộc các chủ đề khác nhau**"
      ],
      "metadata": {
        "id": "VnR6NrY3r8af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chọn danh sách các chủ đề của tài liệu/văn bản cho thử nghiệm\n",
        "topics = [\n",
        "    'the-thao',\n",
        "    'giao-duc',\n",
        "    'khoa-hoc'\n",
        "]\n",
        "\n",
        "# Tạo một tập dữ liệu thử nghiệm gồm các tài liệu/văn bản thuộc về 2-3 chủ đề\n",
        "# Cấu trúc dữ liệu dạng list - lưu thông tin danh sách các tài liệu/văn bản thuộc chủ đề khác nhau\n",
        "# Mỗi tài liệu/văn bản sẽ tổ chức dạng 1 tuple với: (topic, nội_dung_văn_bản, danh_sách_token)\n",
        "D = []"
      ],
      "metadata": {
        "id": "qEh2Spr4r5z7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành viết một số hàm hỗ trợ cho việc đọc dữ liệu, xử lý và tách từ trong tiếng Việt.**"
      ],
      "metadata": {
        "id": "_8U0i3aGsIPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Viết hàm tiền xử lý và tách từ tiếng Việt\n",
        "def preprocess(doc):\n",
        "  # Tiến hành xử lý các lỗi từ/câu, dấu câu, v.v. trong tiếng Việt với hàm text_normalize\n",
        "  normalized_doc = text_normalize(doc)\n",
        "  # Tiến hành tách từ\n",
        "  tokens = word_tokenize(normalized_doc)\n",
        "  # Tiến hành kết hợp các từ ghép trong tiếng Việt bằng '_'\n",
        "  combined_tokens = [token.replace(' ', '_') for token in tokens]\n",
        "  return (normalized_doc, combined_tokens)\n",
        "\n",
        "# Viết hàm lấy danh sách các văn bản/tài liệu thuộc các chủ đề khác nhau\n",
        "def fetch_doc_by_topic(topic):\n",
        "  data_root_dir_path = '/content/IRS/data/vnexpress/{}'.format(topic)\n",
        "  docs = []\n",
        "  for file_name in os.listdir(data_root_dir_path):\n",
        "    file_path = os.path.join(data_root_dir_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "      lines = []\n",
        "      for line in f:\n",
        "        line = line.lower().strip()\n",
        "        lines.append(line)\n",
        "    doc = \" \".join(lines)\n",
        "    clean_doc = re.sub('\\W+',' ', doc)\n",
        "    (normalized_doc, tokens) = preprocess(clean_doc)\n",
        "    docs.append((topic, normalized_doc, tokens))\n",
        "  return docs"
      ],
      "metadata": {
        "id": "gHT1G7dKsLwk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành tạo tập dữ liệu thử nghiệm với các tài liệu/văn bản thuộc danh sách chủ đề [topics] đã lựa chọn bên trên**"
      ],
      "metadata": {
        "id": "DPbmvZv2sQGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu trúc dữ liệu dictionary để lưu thông tin chủ đề-tài liệu, nhằm hỗ trợ cho việc tìm kiếm nhanh\n",
        "topic_doc_idxes_dict = {}\n",
        "doc_idx_topic_dict = {}\n",
        "\n",
        "# Duyệt qua từng chủ đề\n",
        "doc_idx = 0\n",
        "for topic in topics:\n",
        "  current_topic_docs = fetch_doc_by_topic(topic)\n",
        "  topic_doc_idxes_dict[topic] = []\n",
        "  for (topic, normalized_doc, tokens) in current_topic_docs:\n",
        "    topic_doc_idxes_dict[topic].append(doc_idx)\n",
        "    doc_idx_topic_dict[doc_idx] = topic\n",
        "    doc_idx+=1\n",
        "  D += current_topic_docs\n",
        "\n",
        "doc_size = len(D)\n",
        "\n",
        "print('Hoàn tất, tổng số lượng tài liệu/văn bản đã lấy: [{}]'.format(doc_size))\n",
        "for topic in topic_doc_idxes_dict.keys():\n",
        "  print(' - Chủ đề [{}] có [{}] tài liệu/văn bản.'.format(topic, len(topic_doc_idxes_dict[topic])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xH9jCaRsR2s",
        "outputId": "076aa8c1-b1ed-4c0c-db6a-22bbde350782"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hoàn tất, tổng số lượng tài liệu/văn bản đã lấy: [109]\n",
            " - Chủ đề [the-thao] có [39] tài liệu/văn bản.\n",
            " - Chủ đề [giao-duc] có [35] tài liệu/văn bản.\n",
            " - Chủ đề [khoa-hoc] có [35] tài liệu/văn bản.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiến hành biến đổi các tài liệu/văn bản trong tập (D) về dạng các TF-IDF vectors - trong bài thực hành này chúng ta sẽ sử dụng thư viện Scikit-Learn (TfidfVectorizer) https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html**"
      ],
      "metadata": {
        "id": "Suyhqc2Pswy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo đối tượng TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Chúng ta sẽ tạo ra một tập danh sách các tài liệu/văn bản dạng list đơn giản để thư viện Scikit-Learn có thể đọc được\n",
        "sk_docs = []\n",
        "\n",
        "# Duyệt qua từng tài liệu/văn bản có trong (D)\n",
        "for (topic, normalized_doc, tokens) in D:\n",
        "  # Chúng ta sẽ nối các từ/tokens đã được tách để làm thành một văn bản hoàn chỉnh\n",
        "  text = ' '.join(tokens)\n",
        "  sk_docs.append(text)\n",
        "\n",
        "# Tiến hành chuyển đổi các tài liệu/văn bản về dạng các TF-IDF vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(sk_docs)\n",
        "\n",
        "# Chuyển ma trận tfidf_matrix từ dạng cấu trúc thưa sang dạng đầy đủ để thuận tiện cho việc tính toán\n",
        "tfidf_matrix = tfidf_matrix.todense()"
      ],
      "metadata": {
        "id": "2abfe7lCsxik"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QUÁ TRÌNH HUẤN LUYỆN: từ 3 chủ đề: the-thao, giao-duc và khoa-hoc chúng ta đã xác định bên trên cùng với các tài liệu/văn bản đã được chuyển về dạng vector TF-IDF - chúng ta tiến hành xác định các vector mẫu (prototype vector) ($\\vec{p}$) cho từng chủ đề**"
      ],
      "metadata": {
        "id": "RXVeieYEtYDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_p_vector_dict = {}\n",
        "\n",
        "for topic in topic_doc_idxes_dict.keys():\n",
        "  print(f'Xây dựng vector mẫu (p) cho chủ đề: [{topic}]...')\n",
        "  current_topic_doc_idxes = topic_doc_idxes_dict[topic]\n",
        "  p_vector = np.zeros(tfidf_matrix[0].shape)\n",
        "  for doc_idx in current_topic_doc_idxes:\n",
        "    doc_tfidf_vector = tfidf_matrix[doc_idx]\n",
        "    p_vector += doc_tfidf_vector\n",
        "  topic_p_vector_dict[topic] = p_vector\n",
        "  print('Hoàn tất !\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkxoSHdut6pu",
        "outputId": "9088b2e3-7f21-4caa-aa15-ed261c9201ef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xây dựng vector mẫu (p) cho chủ đề: [the-thao]...\n",
            "Hoàn tất !\n",
            "\n",
            "Xây dựng vector mẫu (p) cho chủ đề: [giao-duc]...\n",
            "Hoàn tất !\n",
            "\n",
            "Xây dựng vector mẫu (p) cho chủ đề: [khoa-hoc]...\n",
            "Hoàn tất !\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QUÁ TRÌNH KIỂM THỬ: chúng ta thử dùng mô hình đã được huấn luyện hoàn tất nhằm xác định chủ đề/nhãn lớp cho 1 tài liệu/văn bản mới có nội dung như bên dưới**"
      ],
      "metadata": {
        "id": "qIZY_U3BvbRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Viết hàm giúp chuyển đổi một văn bản mới về dạng tfidf vector\n",
        "def parse_text(text):\n",
        "  (normalized_doc, combined_tokens) = preprocess(text)\n",
        "  parsed_text = ' '.join(combined_tokens)\n",
        "  text_tfidf_vector = vectorizer.transform([parsed_text])[0].todense()\n",
        "  return text_tfidf_vector\n",
        "\n",
        "test_doc = 'Trừ La Liga, bốn giải lớn còn lại của châu Âu đều thi đấu cuối tuần này trước khi các CLB nhả người cho đội tuyển dịp World Cup 2022. Man City đóng góp năm cầu thủ trong danh sách tuyển Anh dự World Cup 2022, nhiều nhất trong số các CLB Ngoại hạng Anh. Dựa vào những gì các học trò Pep Guardiola đang thể hiện, đó không phải bất ngờ. Nhưng HLV Brentford Thomas Frank cho rằng học trò của ông, tiền đạo Ivan Toney, xứng đáng không kém.'\n",
        "\n",
        "test_doc_tfidf_vector = parse_text(test_doc)\n",
        "\n",
        "test_doc_topic_sim_dict = {}\n",
        "\n",
        "print('Xác định mức độ tương đồng giữa tài liệu và các chủ đề/lớp...')\n",
        "for topic in topic_p_vector_dict.keys():\n",
        "  p_vector = topic_p_vector_dict[topic]\n",
        "  cs_sim = 1 - distance.cosine(p_vector, test_doc_tfidf_vector)\n",
        "  test_doc_topic_sim_dict[topic] = cs_sim\n",
        "  print(f' - Mức độ tương đồng giữa tài liệu và chủ đề [{topic}] là: [{cs_sim:.6f}]')\n",
        "\n",
        "sorted_test_doc_topic_sim_dict = dict(sorted(test_doc_topic_sim_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "print(f'Chủ đề/lớp của tài liệu/văn bản được xác định là: [{list(sorted_test_doc_topic_sim_dict.keys())[0]}]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2fd3J3HvpeH",
        "outputId": "d1a29fba-23e6-466c-98f2-286cb0c712a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xác định mức độ tương đồng giữa tài liệu và các chủ đề/lớp...\n",
            " - Mức độ tương đồng giữa tài liệu và chủ đề [the-thao] là: [0.276587]\n",
            " - Mức độ tương đồng giữa tài liệu và chủ đề [giao-duc] là: [0.170873]\n",
            " - Mức độ tương đồng giữa tài liệu và chủ đề [khoa-hoc] là: [0.148787]\n",
            "Chủ đề/lớp của tài liệu/văn bản được xác định là: [the-thao]\n"
          ]
        }
      ]
    }
  ]
}